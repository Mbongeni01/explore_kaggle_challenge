{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Change EDSA challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mbongenimlotha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mbongenimlotha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import TreebankWordTokenizer, SnowballStemmer, PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "\n",
    "\n",
    "import string\n",
    "import urllib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'train.csv')\n",
    "test_df = pd.read_csv(r'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15819 entries, 0 to 15818\n",
      "Data columns (total 3 columns):\n",
      "sentiment    15819 non-null int64\n",
      "message      15819 non-null object\n",
      "tweetid      15819 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 370.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    8530\n",
       " 2    3640\n",
       " 0    2353\n",
       "-1    1296\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is quite imbalanced, we would like to do some sampling so that our model won't overfit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = train_df['sentiment'].value_counts().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [train_df]\n",
    "for class_index, group in train_df.groupby('sentiment'):\n",
    "    lst.append(group.sample(max_size - len(group), replace=True))\n",
    "train_df_new = pd.concat(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a more balanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    8530\n",
       " 2    8530\n",
       " 1    8530\n",
       " 0    8530\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_new['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid\n",
       "0  Europe will now be looking to China to make su...   169760\n",
       "1  Combine this with the polling of staffers re c...    35326\n",
       "2  The scary, unimpeachable evidence that climate...   224985\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10546 entries, 0 to 10545\n",
      "Data columns (total 2 columns):\n",
      "message    10546 non-null object\n",
      "tweetid    10546 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 164.9+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine our train and test datasets for the purposes of perprocessing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbongenimlotha/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    }
   ],
   "source": [
    "full_df = train_df_new.append(test_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>34120</td>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169760</td>\n",
       "      <td>europ will now look china make sure that not a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34121</td>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35326</td>\n",
       "      <td>combin thi with the poll staffer climat chang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34122</td>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>224985</td>\n",
       "      <td>the scari unimpeach evid that climat chang alr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34123</td>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>476263</td>\n",
       "      <td>putin got you too jill trump doesn believ clim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34124</td>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>872928</td>\n",
       "      <td>femal orgasm caus global warm sarcast republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44661</td>\n",
       "      <td>RT @BrittanyBohrer: Brb, writing a poem about ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>895714</td>\n",
       "      <td>brb write poem about climat chang climatechang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44662</td>\n",
       "      <td>2016: the year climate change came home: Durin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>875167</td>\n",
       "      <td>the year climat chang came home dure the hotte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44663</td>\n",
       "      <td>RT @loop_vanuatu: Pacific countries positive a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78329</td>\n",
       "      <td>pacif countri posit about fiji lead the global...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44664</td>\n",
       "      <td>RT @xanria_00018: You’re so hot, you must be t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>867455</td>\n",
       "      <td>you hot you must the caus for global warm aldu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44665</td>\n",
       "      <td>RT @chloebalaoing: climate change is a global ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>470892</td>\n",
       "      <td>climat chang global issu that onli get wors ea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10546 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 message  sentiment  tweetid  \\\n",
       "34120  Europe will now be looking to China to make su...        NaN   169760   \n",
       "34121  Combine this with the polling of staffers re c...        NaN    35326   \n",
       "34122  The scary, unimpeachable evidence that climate...        NaN   224985   \n",
       "34123  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...        NaN   476263   \n",
       "34124  RT @FakeWillMoore: 'Female orgasms cause globa...        NaN   872928   \n",
       "...                                                  ...        ...      ...   \n",
       "44661  RT @BrittanyBohrer: Brb, writing a poem about ...        NaN   895714   \n",
       "44662  2016: the year climate change came home: Durin...        NaN   875167   \n",
       "44663  RT @loop_vanuatu: Pacific countries positive a...        NaN    78329   \n",
       "44664  RT @xanria_00018: You’re so hot, you must be t...        NaN   867455   \n",
       "44665  RT @chloebalaoing: climate change is a global ...        NaN   470892   \n",
       "\n",
       "                                           clean_message  \n",
       "34120  europ will now look china make sure that not a...  \n",
       "34121  combin thi with the poll staffer climat chang ...  \n",
       "34122  the scari unimpeach evid that climat chang alr...  \n",
       "34123  putin got you too jill trump doesn believ clim...  \n",
       "34124   femal orgasm caus global warm sarcast republican  \n",
       "...                                                  ...  \n",
       "44661  brb write poem about climat chang climatechang...  \n",
       "44662  the year climat chang came home dure the hotte...  \n",
       "44663  pacif countri posit about fiji lead the global...  \n",
       "44664  you hot you must the caus for global warm aldu...  \n",
       "44665  climat chang global issu that onli get wors ea...  \n",
       "\n",
       "[10546 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = train_df_new.shape[0]\n",
    "full_df[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44666 entries, 0 to 44665\n",
      "Data columns (total 3 columns):\n",
      "message      44666 non-null object\n",
      "sentiment    34120 non-null float64\n",
      "tweetid      44666 non-null int64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at what a typical tweet looks like to get an idea of what we can expect to see from the other tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PolySciMajor EPA chief doesn't think carbon dioxide is main cause of global warming and.. wait, what!? https://t.co/yeLvcEFXkC via @mashable\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['message'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the tweet above, we see that it contains the following:\n",
    "- Punctuation\n",
    "- Capital letters\n",
    "- Special characters\n",
    "- web link/url\n",
    "\n",
    "We can expect to find the listed items in all the tweets and we need to remove these to clean our tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start cleaning our data by removing patterns we expect to find in our tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(text, pattern):\n",
    "    \"\"\"This function removes patters within\n",
    "    text\"\"\"\n",
    "    r = re.findall(pattern, text)\n",
    "    for i in r:\n",
    "        text = re.sub(i, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove twitter handles \n",
    "handle_pattern = '@[\\w]*'\n",
    "full_df['clean_message'] = np.vectorize(remove_pattern)(full_df['message'], handle_pattern)\n",
    "\n",
    "#remove urls\n",
    "url_pattern = 'https?://[A-Za-z./]*'\n",
    "full_df['clean_message'] = np.vectorize(remove_pattern)(full_df['clean_message'], url_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters, numbers, punctuations\n",
    "pattern = '[^a-zA-Z#]'\n",
    "full_df['clean_message'] = full_df['clean_message'].str.replace(pattern, \" \")\n",
    "\n",
    "full_df['clean_message'] = full_df['clean_message'].apply(lambda x: ' '.join([word for word in x.split() if len(word)>2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>625221</td>\n",
       "      <td>PolySciMajor EPA chief doesn think carbon diox...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126103</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>698562</td>\n",
       "      <td>Researchers say have three years act climate c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>573736</td>\n",
       "      <td>#TodayinMaker# WIRED was pivotal year the war ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>466954</td>\n",
       "      <td>and racist sexist climate change denying bigot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  sentiment  tweetid  \\\n",
       "0  PolySciMajor EPA chief doesn't think carbon di...        1.0   625221   \n",
       "1  It's not like we lack evidence of anthropogeni...        1.0   126103   \n",
       "2  RT @RawStory: Researchers say we have three ye...        2.0   698562   \n",
       "3  #TodayinMaker# WIRED : 2016 was a pivotal year...        1.0   573736   \n",
       "4  RT @SoyNovioDeTodas: It's 2016, and a racist, ...        1.0   466954   \n",
       "\n",
       "                                       clean_message  \n",
       "0  PolySciMajor EPA chief doesn think carbon diox...  \n",
       "1  not like lack evidence anthropogenic global wa...  \n",
       "2  Researchers say have three years act climate c...  \n",
       "3  #TodayinMaker# WIRED was pivotal year the war ...  \n",
       "4  and racist sexist climate change denying bigot...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so now we've removed everything we don't want in our tweets, but we see that some special characters have slipped through. Let's see how we can deal with that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['clean_message'] = full_df['clean_message'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>625221</td>\n",
       "      <td>PolySciMajor EPA chief doesn think carbon diox...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126103</td>\n",
       "      <td>not like lack evidence anthropogenic global wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>698562</td>\n",
       "      <td>Researchers say have three years act climate c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>573736</td>\n",
       "      <td>TodayinMaker WIRED was pivotal year the war cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>466954</td>\n",
       "      <td>and racist sexist climate change denying bigot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  sentiment  tweetid  \\\n",
       "0  PolySciMajor EPA chief doesn't think carbon di...        1.0   625221   \n",
       "1  It's not like we lack evidence of anthropogeni...        1.0   126103   \n",
       "2  RT @RawStory: Researchers say we have three ye...        2.0   698562   \n",
       "3  #TodayinMaker# WIRED : 2016 was a pivotal year...        1.0   573736   \n",
       "4  RT @SoyNovioDeTodas: It's 2016, and a racist, ...        1.0   466954   \n",
       "\n",
       "                                       clean_message  \n",
       "0  PolySciMajor EPA chief doesn think carbon diox...  \n",
       "1  not like lack evidence anthropogenic global wa...  \n",
       "2  Researchers say have three years act climate c...  \n",
       "3  TodayinMaker WIRED was pivotal year the war cl...  \n",
       "4  and racist sexist climate change denying bigot...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nooiiiccee! Seems as if we've gotten some of those stubborn special characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to tokenise our cleaned tweets and make it ready for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = TreebankWordTokenizer()\n",
    "full_df['clean_message'] = full_df['clean_message'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [PolySciMajor, EPA, chief, doesn, think, carbo...\n",
       "1    [not, like, lack, evidence, anthropogenic, glo...\n",
       "2    [Researchers, say, have, three, years, act, cl...\n",
       "3    [TodayinMaker, WIRED, was, pivotal, year, the,...\n",
       "4    [and, racist, sexist, climate, change, denying...\n",
       "Name: clean_message, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['clean_message'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through stemming, we basically transform the words we have in our tweets to their root words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [polyscimajor, epa, chief, doesn, think, carbo...\n",
       "1        [not, like, lack, evid, anthropogen, global, w...\n",
       "2        [research, say, have, three, year, act, climat...\n",
       "3        [todayinmak, wire, wa, pivot, year, the, war, ...\n",
       "4        [and, racist, sexist, climat, chang, deni, big...\n",
       "                               ...                        \n",
       "44661    [brb, write, poem, about, climat, chang, clima...\n",
       "44662    [the, year, climat, chang, came, home, dure, t...\n",
       "44663    [pacif, countri, posit, about, fiji, lead, the...\n",
       "44664    [you, hot, you, must, the, caus, for, global, ...\n",
       "44665    [climat, chang, global, issu, that, onli, get,...\n",
       "Name: clean_message, Length: 44666, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "full_df['clean_message'] = full_df['clean_message'].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "full_df['clean_message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use lemmatization to group words of similar meaning together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [polyscimajor, epa, chief, doesn, think, carbo...\n",
       "1        [not, like, lack, evid, anthropogen, global, w...\n",
       "2        [research, say, have, three, year, act, climat...\n",
       "3        [todayinmak, wire, wa, pivot, year, the, war, ...\n",
       "4        [and, racist, sexist, climat, chang, deni, big...\n",
       "                               ...                        \n",
       "44661    [brb, write, poem, about, climat, chang, clima...\n",
       "44662    [the, year, climat, chang, came, home, dure, t...\n",
       "44663    [pacif, countri, posit, about, fiji, lead, the...\n",
       "44664    [you, hot, you, must, the, caus, for, global, ...\n",
       "44665    [climat, chang, global, issu, that, onli, get,...\n",
       "Name: clean_message, Length: 44666, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "full_df['clean_message'] = full_df['clean_message'].apply(lambda x: [lemmatizer.lemmatize(i) for i in x])\n",
    "full_df['clean_message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining the words of each tweet together again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbongenimlotha/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(full_df['clean_message'])):\n",
    "    full_df['clean_message'][i] = ' '.join(full_df['clean_message'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we want to transform what is essentially a list of words into a feature set that is usable by a classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_count(words, word_dict={}):\n",
    "    \"\"\" this function takes in a list of words and returns a dictionary \n",
    "        with each word as a key, and the value represents the number of \n",
    "        times that word appeared\"\"\"\n",
    "    for word in words:\n",
    "        if word in word_dict.keys():\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = full_df['clean_message'].apply(tokeniser.tokenize)\n",
    "\n",
    "# remove stopwords\n",
    "tokens_less_stopwords = [word for i in tokens for word in i if word not in stopwords.words('english')]\n",
    "\n",
    "# create bag of words\n",
    "bag_of_words = bag_of_words_count(tokens_less_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bag_of_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-211101f8aacc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbag_of_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bag_of_words' is not defined"
     ]
    }
   ],
   "source": [
    "bag_of_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting our full dataframe back into their origional train and test dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = full_df[:n]\n",
    "df_test = full_df[n:]\n",
    "X = df_train['clean_message']\n",
    "y = df_train['sentiment'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=2, stop_words=\"english\")\n",
    "X_vectorized = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X_vectorized.toarray(),y,test_size=.25, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbongenimlotha/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9311042840373558"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, rfc_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(C=0.8, verbose=0)\n",
    "lsvc.fit(X_train,  y_train)\n",
    "lsvc_pred = lsvc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9424828032792862"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, lsvc_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Linear SVC model seems to produce the highest macro f1 score, lets see if we can't improve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.97      0.99      0.98      2131\n",
      "           0       0.94      0.97      0.96      2133\n",
      "           1       0.94      0.85      0.89      2150\n",
      "           2       0.92      0.96      0.94      2116\n",
      "\n",
      "    accuracy                           0.94      8530\n",
      "   macro avg       0.94      0.94      0.94      8530\n",
      "weighted avg       0.94      0.94      0.94      8530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "print(classification_report(y_test, lsvc_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing our Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test['clean_message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vectorized = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbongenimlotha/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_test['sentiment'] = lsvc.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['tweetid', 'sentiment']].to_csv('test_predict.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
